#!/usr/bin/env python3
"""
Fixed SEO Audit Script - ì •í™•í•œ ì •ê·œì‹ìœ¼ë¡œ SEO ì ê²€
"""

import os
import re
import json

def analyze_seo_fixed(html_content, file_path):
    """ê°œì„ ëœ SEO ë¶„ì„ - ì •í™•í•œ ì •ê·œì‹ ì‚¬ìš©"""
    issues = []
    score = 0
    max_score = 100
    
    # íŒŒì¼ ê²½ë¡œì—ì„œ URL ìƒì„±
    base_dir = os.path.abspath(os.path.dirname(__file__))
    try:
        relative_path = os.path.relpath(file_path, start=base_dir)
    except Exception:
        relative_path = file_path
    
    # 1. DOCTYPE ê²€ì‚¬ (5ì )
    if re.search(r'<!DOCTYPE\s+html>', html_content, re.IGNORECASE):
        score += 5
    else:
        issues.append("âŒ DOCTYPE ì„ ì–¸ ëˆ„ë½")
    
    # 2. HTML lang ì†ì„± (5ì )
    if re.search(r'<html[^>]+lang\s*=', html_content, re.IGNORECASE):
        score += 5
    else:
        issues.append("âŒ HTML lang ì†ì„± ëˆ„ë½")
    
    # 3. Title íƒœê·¸ (15ì )
    title_match = re.search(r'<title[^>]*>([^<]*)</title>', html_content, re.IGNORECASE)
    if title_match:
        title_text = title_match.group(1).strip()
        title_length = len(title_text)
        if 30 <= title_length <= 60:
            score += 15
        elif title_length > 0:
            score += 10
            if title_length < 30:
                issues.append(f"âš ï¸ ì œëª©ì´ ë„ˆë¬´ ì§§ìŒ ({title_length}ì)")
            else:
                issues.append(f"âš ï¸ ì œëª©ì´ ë„ˆë¬´ ê¸¸ìŒ ({title_length}ì)")
    else:
        issues.append("âŒ ì œëª© íƒœê·¸ ëˆ„ë½")
    
    # 4. Meta description (15ì ) - ìˆ˜ì •ëœ ì •ê·œì‹
    # ë‹¤ì–‘í•œ ë©”íƒ€ ì„¤ëª… í˜•ì‹ì„ ëª¨ë‘ ìºì¹˜
    meta_desc_patterns = [
        r'<meta\s+name=["\']description["\']\s+content=["\']([^"\']*)["\']',
        r'<meta\s+content=["\']([^"\']*?)["\']\s+name=["\']description["\']',
        r'<meta[^>]*name\s*=\s*["\']description["\'][^>]*content\s*=\s*["\']([^"\']*)["\']',
        r'<meta[^>]*content\s*=\s*["\']([^"\']*?)["\']\s*name\s*=\s*["\']description["\']'
    ]
    
    desc_text = ""
    for pattern in meta_desc_patterns:
        meta_desc_match = re.search(pattern, html_content, re.IGNORECASE)
        if meta_desc_match:
            desc_text = meta_desc_match.group(1).strip()
            break
    
    if desc_text:
        desc_length = len(desc_text)
        if 120 <= desc_length <= 160:
            score += 15
        elif desc_length > 0:
            score += 10
            if desc_length < 120:
                issues.append(f"âš ï¸ ë©”íƒ€ ì„¤ëª…ì´ ë„ˆë¬´ ì§§ìŒ ({desc_length}ì)")
            else:
                issues.append(f"âš ï¸ ë©”íƒ€ ì„¤ëª…ì´ ë„ˆë¬´ ê¸¸ìŒ ({desc_length}ì)")
    else:
        issues.append("âŒ ë©”íƒ€ ì„¤ëª… ëˆ„ë½")
    
    # 5. Meta keywords (5ì )
    if re.search(r'<meta[^>]+name\s*=\s*["\']keywords["\']', html_content, re.IGNORECASE):
        score += 5
    else:
        issues.append("âš ï¸ ë©”íƒ€ í‚¤ì›Œë“œ ëˆ„ë½")
    
    # 6. Viewport meta (5ì )
    if re.search(r'<meta[^>]+name\s*=\s*["\']viewport["\']', html_content, re.IGNORECASE):
        score += 5
    else:
        issues.append("âŒ ëª¨ë°”ì¼ ë·°í¬íŠ¸ íƒœê·¸ ëˆ„ë½")
    
    # 7. Charset (5ì )
    if re.search(r'<meta[^>]+charset\s*=', html_content, re.IGNORECASE):
        score += 5
    else:
        issues.append("âŒ ë¬¸ì ì¸ì½”ë”© ì„¤ì • ëˆ„ë½")
    
    # 8. Open Graph íƒœê·¸ (15ì ) - ë” ì •í™•í•œ ê²€ì‚¬
    og_tags = ['og:title', 'og:description', 'og:image', 'og:url', 'og:type']
    og_found = []
    for tag in og_tags:
        if re.search(f'<meta[^>]+property\s*=\s*["\']?{re.escape(tag)}["\']?', html_content, re.IGNORECASE):
            og_found.append(tag)
        else:
            issues.append(f"âš ï¸ {tag} ëˆ„ë½")
    score += len(og_found) * 3
    
    # 9. Twitter Card íƒœê·¸ (15ì ) - ë” ì •í™•í•œ ê²€ì‚¬  
    twitter_tags = ['twitter:card', 'twitter:title', 'twitter:description', 'twitter:image', 'twitter:site']
    twitter_found = []
    for tag in twitter_tags:
        if re.search(f'<meta[^>]+name\s*=\s*["\']?{re.escape(tag)}["\']?', html_content, re.IGNORECASE):
            twitter_found.append(tag)
        else:
            issues.append(f"âš ï¸ {tag} ëˆ„ë½")
    score += len(twitter_found) * 3
    
    # 10. JSON-LD êµ¬ì¡°í™” ë°ì´í„° (15ì )
    json_ld_matches = re.findall(r'<script[^>]+type\s*=\s*["\']application/ld\+json["\'][^>]*>(.*?)</script>', html_content, re.DOTALL | re.IGNORECASE)
    if json_ld_matches:
        valid_schemas = 0
        for match in json_ld_matches:
            try:
                json_data = json.loads(match.strip())
                if '@context' in json_data and '@type' in json_data:
                    valid_schemas += 1
            except:
                pass
        if valid_schemas >= 2:
            score += 15
        elif valid_schemas >= 1:
            score += 10
        else:
            score += 5
    else:
        issues.append("âŒ JSON-LD êµ¬ì¡°í™” ë°ì´í„° ëˆ„ë½")
    
    # 11. Canonical URL (5ì )
    if re.search(r'<link[^>]+rel\s*=\s*["\']canonical["\']', html_content, re.IGNORECASE):
        score += 5
    else:
        issues.append("âš ï¸ Canonical URL ëˆ„ë½")
    
    # 12. ì´ë¯¸ì§€ alt ì†ì„± (10ì )
    img_tags = re.findall(r'<img[^>]*>', html_content, re.IGNORECASE)
    if img_tags:
        imgs_with_alt = [img for img in img_tags if re.search(r'alt\s*=', img, re.IGNORECASE)]
        alt_ratio = len(imgs_with_alt) / len(img_tags) if img_tags else 0
        score += int(alt_ratio * 10)
        if alt_ratio < 1.0:
            missing_alt = len(img_tags) - len(imgs_with_alt)
            issues.append(f"âš ï¸ {missing_alt}ê°œ ì´ë¯¸ì§€ì˜ alt ì†ì„± ëˆ„ë½")
    
    return {
        'score': min(score, max_score),  # ìµœëŒ€ 100ì  ì œí•œ
        'issues': issues,
        'raw_score': score,
        'max_score': max_score,
        'details': {
            'title_length': len(title_match.group(1).strip()) if title_match else 0,
            'meta_desc_length': len(desc_text) if desc_text else 0,
            'meta_desc_found': bool(desc_text),
            'images_count': len(img_tags) if 'img_tags' in locals() else 0,
            'images_with_alt': len(imgs_with_alt) if 'imgs_with_alt' in locals() else 0,
            'json_ld_count': len(json_ld_matches) if 'json_ld_matches' in locals() else 0,
            'og_tags_found': len(og_found),
            'twitter_tags_found': len(twitter_found),
        }
    }

def scan_all_html_files():
    """ëª¨ë“  HTML íŒŒì¼ì„ ìŠ¤ìº”í•˜ì—¬ ì •í™•í•œ SEO ë¶„ì„"""
    base_path = os.path.abspath(os.path.dirname(__file__))
    
    results = []
    
    for root, dirs, files in os.walk(base_path):
        for file in files:
            if file.endswith('.html'):
                file_path = os.path.join(root, file)
                
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    analysis = analyze_seo_fixed(content, file_path)
                    analysis['file'] = file_path.replace(base_path, '').lstrip('/')
                    results.append(analysis)
                    
                except Exception as e:
                    print(f"Error analyzing {file_path}: {e}")
    
    return results

def generate_detailed_report(results):
    """ìƒì„¸í•œ SEO ë¦¬í¬íŠ¸ ìƒì„±"""
    results.sort(key=lambda x: x['score'])
    
    total_files = len(results)
    avg_score = sum(r['score'] for r in results) / total_files if total_files > 0 else 0
    
    print("=" * 70)
    print("ğŸ” FIXED & DETAILED SEO AUDIT REPORT")
    print("=" * 70)
    print(f"ğŸ“Š ì´ ë¶„ì„ íŒŒì¼: {total_files}ê°œ")
    print(f"ğŸ“ˆ í‰ê·  SEO ì ìˆ˜: {avg_score:.1f}/100")
    print()
    
    # ì ìˆ˜ë³„ ë¶„ë¥˜
    excellent = [r for r in results if r['score'] >= 90]
    good = [r for r in results if 80 <= r['score'] < 90]
    needs_improvement = [r for r in results if r['score'] < 80]
    
    print(f"ğŸ† ìš°ìˆ˜ (90+): {len(excellent)}ê°œ íŒŒì¼ ({len(excellent)/total_files*100:.1f}%)")
    print(f"âœ… ì–‘í˜¸ (80-89): {len(good)}ê°œ íŒŒì¼ ({len(good)/total_files*100:.1f}%)")
    print(f"âš ï¸ ê°œì„  í•„ìš” (<80): {len(needs_improvement)}ê°œ íŒŒì¼ ({len(needs_improvement)/total_files*100:.1f}%)")
    print()
    
    # ë©”íƒ€ ì„¤ëª… í†µê³„
    files_with_meta_desc = [r for r in results if r['details']['meta_desc_found']]
    print(f"ğŸ“ ë©”íƒ€ ì„¤ëª… ë³´ìœ  íŒŒì¼: {len(files_with_meta_desc)}ê°œ ({len(files_with_meta_desc)/total_files*100:.1f}%)")
    print()
    
    # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„
    categories = {
        'romance-test': [r for r in results if 'romance-test' in r['file']],
        'egen-teto': [r for r in results if 'egen-teto' in r['file']],
        'anime-personality': [r for r in results if 'anime-personality' in r['file']],
        'main': [r for r in results if r['file'].count('/') <= 1 and 'index.html' in r['file']],
        'utility': [r for r in results if any(x in r['file'] for x in ['create_', 'generate_', 'google', 'yandex', 'sandbox', 'monitoring'])],
    }
    
    print("ğŸ“‚ ì¹´í…Œê³ ë¦¬ë³„ ìƒì„¸ ì ìˆ˜:")
    print("-" * 50)
    for cat_name, cat_results in categories.items():
        if cat_results:
            cat_avg = sum(r['score'] for r in cat_results) / len(cat_results)
            files_with_desc = sum(1 for r in cat_results if r['details']['meta_desc_found'])
            print(f"ğŸ“ {cat_name}:")
            print(f"   í‰ê·  ì ìˆ˜: {cat_avg:.1f}ì  ({len(cat_results)}ê°œ íŒŒì¼)")  
            print(f"   ë©”íƒ€ ì„¤ëª…: {files_with_desc}ê°œ íŒŒì¼ ë³´ìœ ")
            
            # ì†Œì…œ ë¯¸ë””ì–´ íƒœê·¸ í†µê³„
            avg_og = sum(r['details']['og_tags_found'] for r in cat_results) / len(cat_results)
            avg_twitter = sum(r['details']['twitter_tags_found'] for r in cat_results) / len(cat_results)
            print(f"   Open Graph: í‰ê·  {avg_og:.1f}/5ê°œ íƒœê·¸")
            print(f"   Twitter Card: í‰ê·  {avg_twitter:.1f}/5ê°œ íƒœê·¸")
            print()
    
    # ê°œì„ ì´ í•„ìš”í•œ íŒŒì¼ë“¤ ìƒì„¸ ë¶„ì„
    print("ğŸ”§ ì¦‰ì‹œ ê°œì„  í•„ìš” íŒŒì¼ë“¤:")
    print("-" * 40)
    
    # Twitter Card ëˆ„ë½ íŒŒì¼ë“¤
    twitter_missing = [r for r in results if r['details']['twitter_tags_found'] < 5]
    print(f"ğŸ“± Twitter Card íƒœê·¸ ëˆ„ë½: {len(twitter_missing)}ê°œ íŒŒì¼")
    
    # Open Graph ëˆ„ë½ íŒŒì¼ë“¤  
    og_missing = [r for r in results if r['details']['og_tags_found'] < 5]
    print(f"ğŸ“ˆ Open Graph íƒœê·¸ ëˆ„ë½: {len(og_missing)}ê°œ íŒŒì¼")
    
    # ì œëª© ê¸¸ì´ ë¬¸ì œ íŒŒì¼ë“¤
    title_issues = [r for r in results if r['details']['title_length'] < 30 or r['details']['title_length'] > 60]
    print(f"ğŸ“ ì œëª© ê¸¸ì´ ë¬¸ì œ: {len(title_issues)}ê°œ íŒŒì¼")
    
    return results

if __name__ == "__main__":
    print("ğŸ” ìˆ˜ì •ëœ SEO ìŠ¤í¬ë¦½íŠ¸ë¡œ ì •í™•í•œ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    results = scan_all_html_files()
    generate_detailed_report(results)
    
    # ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥
    output_path = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'seo_fixed_results.json')
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print("\nâœ… ì •í™•í•œ ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ê°€ 'seo_fixed_results.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
