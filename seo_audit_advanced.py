#!/usr/bin/env python3
"""
Advanced SEO Audit Script - ê³ ê¸‰ SEO ì ê²€ ë° ìµœì í™” ë¶„ì„
"""

import os
import re
import json
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import sys

def analyze_seo_advanced(html_content, file_path):
    """ê³ ê¸‰ SEO ë¶„ì„ - ë” ì„¸ë°€í•œ ì ìˆ˜ ê³„ì‚°"""
    soup = BeautifulSoup(html_content, 'html.parser')
    
    issues = []
    score = 0
    max_score = 200  # ë” ì„¸ë°€í•œ ì ìˆ˜ ì²´ê³„
    
    # íŒŒì¼ ê²½ë¡œì—ì„œ URL ìƒì„±
    base_dir = os.path.abspath(os.path.dirname(__file__))
    try:
        relative_path = os.path.relpath(file_path, start=base_dir)
    except Exception:
        relative_path = file_path
    expected_url = f"https://tests.mahalohana-bruce.com/{relative_path}"
    
    # 1. ê¸°ë³¸ HTML êµ¬ì¡° (20ì )
    doctype = str(soup).strip().startswith('<!DOCTYPE html>')
    if doctype:
        score += 10
    else:
        issues.append("âŒ DOCTYPE ì„ ì–¸ ëˆ„ë½")
    
    html_tag = soup.find('html')
    if html_tag and html_tag.get('lang'):
        score += 10
    else:
        issues.append("âŒ HTML lang ì†ì„± ëˆ„ë½")
    
    # 2. ë©”íƒ€ íƒœê·¸ ê²€ì‚¬ (50ì )
    title = soup.find('title')
    if title and title.string:
        title_text = title.string.strip()
        if 30 <= len(title_text) <= 60:
            score += 15
        elif len(title_text) < 30:
            score += 8
            issues.append(f"âš ï¸ ì œëª©ì´ ë„ˆë¬´ ì§§ìŒ ({len(title_text)}ì)")
        else:
            score += 8
            issues.append(f"âš ï¸ ì œëª©ì´ ë„ˆë¬´ ê¸¸ìŒ ({len(title_text)}ì)")
    else:
        issues.append("âŒ ì œëª© íƒœê·¸ ëˆ„ë½")
    
    meta_description = soup.find('meta', attrs={'name': 'description'})
    if meta_description and meta_description.get('content'):
        desc_text = meta_description.get('content').strip()
        if 120 <= len(desc_text) <= 160:
            score += 15
        elif len(desc_text) < 120:
            score += 10
            issues.append(f"âš ï¸ ë©”íƒ€ ì„¤ëª…ì´ ë„ˆë¬´ ì§§ìŒ ({len(desc_text)}ì)")
        else:
            score += 10
            issues.append(f"âš ï¸ ë©”íƒ€ ì„¤ëª…ì´ ë„ˆë¬´ ê¸¸ìŒ ({len(desc_text)}ì)")
    else:
        issues.append("âŒ ë©”íƒ€ ì„¤ëª… ëˆ„ë½")
    
    meta_keywords = soup.find('meta', attrs={'name': 'keywords'})
    if meta_keywords and meta_keywords.get('content'):
        score += 5
    else:
        issues.append("âš ï¸ ë©”íƒ€ í‚¤ì›Œë“œ ëˆ„ë½")
    
    viewport = soup.find('meta', attrs={'name': 'viewport'})
    if viewport:
        score += 10
    else:
        issues.append("âŒ ëª¨ë°”ì¼ ë·°í¬íŠ¸ íƒœê·¸ ëˆ„ë½")
    
    charset = soup.find('meta', attrs={'charset': True})
    if charset:
        score += 5
    else:
        issues.append("âŒ ë¬¸ì ì¸ì½”ë”© ì„¤ì • ëˆ„ë½")
    
    # 3. Open Graph íƒœê·¸ (25ì )
    og_tags = ['og:title', 'og:description', 'og:image', 'og:url', 'og:type']
    og_score = 0
    for tag in og_tags:
        if soup.find('meta', attrs={'property': tag}):
            og_score += 5
        else:
            issues.append(f"âš ï¸ {tag} ëˆ„ë½")
    score += og_score
    
    # 4. Twitter Card íƒœê·¸ (25ì )
    twitter_tags = ['twitter:card', 'twitter:title', 'twitter:description', 'twitter:image', 'twitter:site']
    twitter_score = 0
    for tag in twitter_tags:
        if soup.find('meta', attrs={'name': tag}):
            twitter_score += 5
        else:
            issues.append(f"âš ï¸ {tag} ëˆ„ë½")
    score += twitter_score
    
    # 5. êµ¬ì¡°í™” ë°ì´í„° (30ì )
    json_ld_scripts = soup.find_all('script', {'type': 'application/ld+json'})
    if json_ld_scripts:
        score += 20
        # JSON-LD êµ¬ì¡° ê²€ì¦
        valid_schemas = 0
        for script in json_ld_scripts:
            try:
                json_data = json.loads(script.string)
                if '@context' in json_data and '@type' in json_data:
                    valid_schemas += 1
            except:
                pass
        if valid_schemas >= 2:
            score += 10
        elif valid_schemas >= 1:
            score += 5
    else:
        issues.append("âŒ JSON-LD êµ¬ì¡°í™” ë°ì´í„° ëˆ„ë½")
    
    # 6. Canonical URL (10ì )
    canonical = soup.find('link', attrs={'rel': 'canonical'})
    if canonical and canonical.get('href'):
        score += 10
    else:
        issues.append("âš ï¸ Canonical URL ëˆ„ë½")
    
    # 7. ì´ë¯¸ì§€ ìµœì í™” (20ì )
    images = soup.find_all('img')
    if images:
        images_with_alt = [img for img in images if img.get('alt')]
        alt_ratio = len(images_with_alt) / len(images)
        score += int(alt_ratio * 10)
        if alt_ratio < 1.0:
            missing_alt = len(images) - len(images_with_alt)
            issues.append(f"âš ï¸ {missing_alt}ê°œ ì´ë¯¸ì§€ì˜ alt ì†ì„± ëˆ„ë½")
        
        # ì´ë¯¸ì§€ loading ì†ì„± ê²€ì‚¬
        lazy_images = [img for img in images if img.get('loading') == 'lazy']
        if lazy_images:
            score += 5
        
        # ì´ë¯¸ì§€ í¬ê¸° ì†ì„± ê²€ì‚¬
        sized_images = [img for img in images if img.get('width') and img.get('height')]
        if len(sized_images) == len(images):
            score += 5
    
    # 8. ì„±ëŠ¥ ê´€ë ¨ íƒœê·¸ (10ì )
    if soup.find('link', attrs={'rel': 'preconnect'}):
        score += 3
    if soup.find('link', attrs={'rel': 'dns-prefetch'}):
        score += 2
    if soup.find('script', attrs={'async': True}):
        score += 3
    if soup.find('script', attrs={'defer': True}):
        score += 2
    
    # 9. ë³´ì•ˆ íƒœê·¸ (10ì )
    if soup.find('meta', attrs={'http-equiv': 'Content-Security-Policy'}):
        score += 5
    if soup.find('meta', attrs={'name': 'robots'}):
        score += 5
    
    # ìµœì¢… ì ìˆ˜ë¥¼ 100ì  ë§Œì ìœ¼ë¡œ ë³€í™˜
    final_score = (score / max_score) * 100
    
    return {
        'score': round(final_score, 1),
        'issues': issues,
        'raw_score': score,
        'max_score': max_score,
        'details': {
            'title_length': len(title.string.strip()) if title and title.string else 0,
            'meta_desc_length': len(meta_description.get('content').strip()) if meta_description and meta_description.get('content') else 0,
            'images_count': len(images) if 'images' in locals() else 0,
            'images_with_alt': len(images_with_alt) if 'images_with_alt' in locals() else 0,
            'json_ld_count': len(json_ld_scripts),
            'has_canonical': canonical is not None,
        }
    }

def scan_all_html_files():
    """ëª¨ë“  HTML íŒŒì¼ì„ ìŠ¤ìº”í•˜ì—¬ ê³ ê¸‰ SEO ë¶„ì„"""
    base_path = os.path.abspath(os.path.dirname(__file__))
    
    results = []
    
    for root, dirs, files in os.walk(base_path):
        for file in files:
            if file.endswith('.html'):
                file_path = os.path.join(root, file)
                
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    analysis = analyze_seo_advanced(content, file_path)
                    analysis['file'] = file_path.replace(base_path, '').lstrip('/')
                    results.append(analysis)
                    
                except Exception as e:
                    print(f"Error analyzing {file_path}: {e}")
    
    return results

def generate_advanced_report(results):
    """ê³ ê¸‰ SEO ë¦¬í¬íŠ¸ ìƒì„±"""
    results.sort(key=lambda x: x['score'])
    
    total_files = len(results)
    avg_score = sum(r['score'] for r in results) / total_files if total_files > 0 else 0
    
    print("=" * 60)
    print("ğŸš€ ADVANCED SEO AUDIT REPORT")
    print("=" * 60)
    print(f"ğŸ“Š ì´ ë¶„ì„ íŒŒì¼: {total_files}ê°œ")
    print(f"ğŸ“ˆ í‰ê·  SEO ì ìˆ˜: {avg_score:.1f}/100")
    print()
    
    # ì ìˆ˜ë³„ ë¶„ë¥˜
    excellent = [r for r in results if r['score'] >= 90]
    good = [r for r in results if 80 <= r['score'] < 90]
    needs_improvement = [r for r in results if r['score'] < 80]
    
    print(f"ğŸ† ìš°ìˆ˜ (90+): {len(excellent)}ê°œ íŒŒì¼")
    print(f"âœ… ì–‘í˜¸ (80-89): {len(good)}ê°œ íŒŒì¼")
    print(f"âš ï¸ ê°œì„  í•„ìš” (<80): {len(needs_improvement)}ê°œ íŒŒì¼")
    print()
    
    # ê°€ì¥ ê°œì„ ì´ í•„ìš”í•œ íŒŒì¼ë“¤
    print("ğŸ”§ ê°œì„ ì´ ê°€ì¥ í•„ìš”í•œ íŒŒì¼ë“¤:")
    print("-" * 40)
    for result in results[:10]:  # ì ìˆ˜ê°€ ë‚®ì€ ìƒìœ„ 10ê°œ
        print(f"ğŸ“„ {result['file']}")
        print(f"   ì ìˆ˜: {result['score']}/100")
        print(f"   ì£¼ìš” ë¬¸ì œ: {len(result['issues'])}ê°œ")
        if result['issues'][:3]:  # ìƒìœ„ 3ê°œ ë¬¸ì œë§Œ í‘œì‹œ
            for issue in result['issues'][:3]:
                print(f"   â€¢ {issue}")
        print()
    
    # ê³µí†µ ë¬¸ì œì  ë¶„ì„
    all_issues = []
    for result in results:
        all_issues.extend(result['issues'])
    
    # ë¬¸ì œì  ë¹ˆë„ ê³„ì‚°
    issue_counts = {}
    for issue in all_issues:
        issue_type = issue.split(' ')[1] if ' ' in issue else issue
        issue_counts[issue_type] = issue_counts.get(issue_type, 0) + 1
    
    print("ğŸ“Š ê°€ì¥ í”í•œ SEO ë¬¸ì œë“¤:")
    print("-" * 30)
    sorted_issues = sorted(issue_counts.items(), key=lambda x: x[1], reverse=True)[:10]
    for issue, count in sorted_issues:
        percentage = (count / total_files) * 100
        print(f"â€¢ {issue}: {count}ê°œ íŒŒì¼ ({percentage:.1f}%)")
    
    return results

if __name__ == "__main__":
    print("ğŸ” ê³ ê¸‰ SEO ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    results = scan_all_html_files()
    generate_advanced_report(results)
    
    # ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥
    output_path = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'seo_audit_advanced_results.json')
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print("\nâœ… ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ê°€ 'seo_audit_advanced_results.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
